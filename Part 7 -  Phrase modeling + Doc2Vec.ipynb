{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>awww that bummer you shoulda got david carr of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he can not update his facebook b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dived many times for the ball managed to save ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no it not behaving at all mad why am here beca...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  awww that bummer you shoulda got david carr of...       0\n",
       "1  is upset that he can not update his facebook b...       0\n",
       "2  dived many times for the ball managed to save ...       0\n",
       "3     my whole body feels itchy and like its on fire       0\n",
       "4  no it not behaving at all mad why am here beca...       0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv = 'clean_tweet.csv'\n",
    "my_df = pd.read_csv(csv,index_col=0)\n",
    "my_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1596019 entries, 0 to 1596018\n",
      "Data columns (total 2 columns):\n",
      "text      1596019 non-null object\n",
      "target    1596019 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 24.4+ MB\n"
     ]
    }
   ],
   "source": [
    "my_df.dropna(inplace=True)\n",
    "my_df.reset_index(drop=True,inplace=True)\n",
    "my_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Dev / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = my_df.text\n",
    "y = my_df.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "SEED = 2000\n",
    "x_train, x_validation_and_test, y_train, y_validation_and_test = train_test_split(x, y, test_size=.02, random_state=SEED)\n",
    "x_validation, x_test, y_validation, y_test = train_test_split(x_validation_and_test, y_validation_and_test, test_size=.5, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set has total 1564098 entries with 50.00% negative, 50.00% positive\n",
      "Validation set has total 15960 entries with 50.40% negative, 49.60% positive\n",
      "Test set has total 15961 entries with 50.26% negative, 49.74% positive\n"
     ]
    }
   ],
   "source": [
    "print \"Train set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_train),\n",
    "                                                                             (len(x_train[y_train == 0]) / (len(x_train)*1.))*100,\n",
    "                                                                            (len(x_train[y_train == 1]) / (len(x_train)*1.))*100)\n",
    "print \"Validation set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_validation),\n",
    "                                                                             (len(x_validation[y_validation == 0]) / (len(x_validation)*1.))*100,\n",
    "                                                                            (len(x_validation[y_validation == 1]) / (len(x_validation)*1.))*100)\n",
    "print \"Test set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_test),\n",
    "                                                                             (len(x_test[y_test == 0]) / (len(x_test)*1.))*100,\n",
    "                                                                            (len(x_test[y_test == 1]) / (len(x_test)*1.))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "import multiprocessing\n",
    "from sklearn import utils\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_vectors(model, corpus, size):\n",
    "    vecs = np.zeros((len(corpus), size))\n",
    "    n = 0\n",
    "    for i in corpus.index:\n",
    "        prefix = 'all_' + str(i)\n",
    "        vecs[n] = model.docvecs[prefix]\n",
    "        n += 1\n",
    "    return vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_concat_vectors(model1,model2, corpus, size):\n",
    "    vecs = np.zeros((len(corpus), size))\n",
    "    n = 0\n",
    "    for i in corpus.index:\n",
    "        prefix = 'all_' + str(i)\n",
    "        vecs[n] = np.append(model1.docvecs[prefix],model2.docvecs[prefix])\n",
    "        n += 1\n",
    "    return vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing that can be implemented with Gensim library is phrase detection. It is similar to n-gram, but instead of getting all the n-gram by sliding the window, it detects frequently-used phrases and sticks them together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patrick Harrison has provided a nice explanation of Gensim's phrase modelling in PyData DC 2016.\n",
    "\n",
    "$$\\frac {{count(A B)}-{count_{min}}} {{count(A)} \\times {count(B)}} \\times {N} > {threshold}$$\n",
    "\n",
    "where:\n",
    "\n",
    "- count(A) is the number of times token A appears in the corpus\n",
    "- count(B) is the number of times token B appears in the corpus\n",
    "- count(A B) is the number of times the tokens A B appear in the corpus in order\n",
    "- N is the total size of the corpus vocabulary\n",
    "- count_{min} is a user-defined parameter to ensure that accepted phrases occur a minimum number of times\n",
    "- threshold is a user-defined parameter to control how strong of a relationship between two tokens the model requires before accepting them as a phrase (default threshold used in Gensim's Phrases function is 10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK let's see how this actually works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases\n",
    "from gensim.models.phrases import Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_train = [t.split() for t in x_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By feeding all the tokenized tweets corpus, it will detect the frequently used phrase and connect them together with underbar in the middle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 9s, sys: 5.78 s, total: 3min 14s\n",
      "Wall time: 3min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "phrases = Phrases(tokenized_train)\n",
    "bigram = Phraser(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'the', u'mayor', u'of', u'new_york', u'was', u'there']\n"
     ]
    }
   ],
   "source": [
    "sent = [u'the', u'mayor', u'of', u'new', u'york', u'was', u'there']\n",
    "print(bigram[sent])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the above example, with the tweets corpus it has learned \"New York\" as a frequently used phrase. So now feeding the \"bigram\" with tokens separated \"new\" and \"york\", it will automatically put them together into one word as \"new_york\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'last time with nutella and vanilla ice cream sadface'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[627092]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'last',\n",
       " u'time',\n",
       " u'with',\n",
       " u'nutella',\n",
       " u'and',\n",
       " u'vanilla_ice',\n",
       " u'cream',\n",
       " u'sadface']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram[x_train[627092].split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we check with one of the tweets from the corpus, we can see that the bigram model has recognised \"vanilla_ice\" as a phrase. This is interesting, and I will come back to this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's transform our corpus with this bigram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def labelize_tweets_bg(tweets,label):\n",
    "    result = []\n",
    "    prefix = label\n",
    "    for i, t in zip(tweets.index, tweets):\n",
    "        result.append(LabeledSentence(bigram[t.split()], [prefix + '_%s' % i]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_x = pd.concat([x_train,x_validation,x_test])\n",
    "all_x_w2v_bg = labelize_tweets_bg(all_x, 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After I get the corpus with bigram phrases detected, I went over the same process of Doc2Vec I did with unigram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBOW Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1043062.41it/s]\n"
     ]
    }
   ],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "model_bg_dbow = Doc2Vec(dm=0, size=100, negative=5, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model_bg_dbow.build_vocab([x for x in tqdm(all_x_w2v_bg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1316773.39it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1418144.08it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1335872.41it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1364295.81it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1321912.54it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1313759.16it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1145635.32it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1396075.76it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1365013.55it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1260398.87it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1416309.33it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1304179.97it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1358849.28it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1246421.60it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1298254.09it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1272354.94it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1373550.42it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1391527.11it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1288958.19it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1240264.07it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1356754.26it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1245451.81it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1210698.42it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1157699.08it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1202860.42it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1162206.75it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1131699.62it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1268661.27it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1259788.80it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1266865.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39min 55s, sys: 17min 8s, total: 57min 4s\n",
      "Wall time: 36min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_bg_dbow.train(utils.shuffle([x for x in tqdm(all_x_w2v_bg)]), total_examples=len(all_x_w2v_bg), epochs=1)\n",
    "    model_bg_dbow.alpha -= 0.002\n",
    "    model_bg_dbow.min_alpha = model_bg_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_vecs_dbow_bg = get_vectors(model_bg_dbow, x_train, 100)\n",
    "validation_vecs_dbow_bg = get_vectors(model_bg_dbow, x_validation, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.3 s, sys: 7.28 s, total: 30.6 s\n",
      "Wall time: 32.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dbow_bg, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73790726817042607"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(validation_vecs_dbow_bg, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_bg_dbow.save('d2v_model_bg_dbow.doc2vec')\n",
    "model_bg_dbow = Doc2Vec.load('d2v_model_bg_dbow.doc2vec')\n",
    "model_bg_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DMC Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1248968.03it/s]\n"
     ]
    }
   ],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "model_bg_dmc = Doc2Vec(dm=1, dm_concat=1, size=100, window=2, negative=5, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model_bg_dmc.build_vocab([x for x in tqdm(all_x_w2v_bg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1203649.63it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1350325.81it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1350213.87it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1297423.50it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1385883.24it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1244686.46it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1425682.66it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1312333.11it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1295467.84it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1287446.27it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1355128.59it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1261521.40it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1364672.94it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1339324.22it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1403120.92it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1253678.17it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1358576.53it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1345735.21it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1186094.01it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1395871.99it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1170088.95it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1318138.26it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1278159.70it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1357162.45it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1266130.01it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1257791.48it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1277595.96it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1273980.93it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1264662.30it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1288731.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48min 8s, sys: 17min 9s, total: 1h 5min 17s\n",
      "Wall time: 36min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_bg_dmc.train(utils.shuffle([x for x in tqdm(all_x_w2v_bg)]), total_examples=len(all_x_w2v_bg), epochs=1)\n",
    "    model_bg_dmc.alpha -= 0.002\n",
    "    model_bg_dmc.min_alpha = model_bg_dmc.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'ny', 0.7682234644889832),\n",
       " (u'chicago', 0.7522180676460266),\n",
       " (u'berlin', 0.7467372417449951),\n",
       " (u'texas', 0.7409263253211975),\n",
       " (u'paris', 0.7380017638206482),\n",
       " (u'nashville', 0.7352598309516907),\n",
       " (u'nyc', 0.7345788478851318),\n",
       " (u'london', 0.7340636253356934),\n",
       " (u'boston', 0.7281099557876587),\n",
       " (u'florida', 0.726203203201294)]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_bg_dmc.most_similar('new_york')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since now we have bigram phrase detected corpus, if we look for the most similar words to \"new_york\", the most similar word for 'new_york' is 'ny' which is pretty amazing, and you can also see other city names as 'chicago', 'berlin', etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_vecs_dmc_bg = get_vectors(model_bg_dmc, x_train, 100)\n",
    "validation_vecs_dmc_bg = get_vectors(model_bg_dmc, x_validation, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.1 s, sys: 7.93 s, total: 22 s\n",
      "Wall time: 25.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dmc_bg, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64974937343358397"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(validation_vecs_dmc_bg, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_bg_dmc.save('d2v_model_bg_dmc.doc2vec')\n",
    "model_bg_dmc = Doc2Vec.load('d2v_model_bg_dmc.doc2vec')\n",
    "model_bg_dmc.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DMM Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1015076.49it/s]\n"
     ]
    }
   ],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "model_bg_dmm = Doc2Vec(dm=1, dm_mean=1, size=100, window=4, negative=5, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model_bg_dmm.build_vocab([x for x in tqdm(all_x_w2v_bg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1177680.86it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1298185.36it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1351305.73it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1407373.19it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1402785.14it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1301223.06it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1299343.96it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1342313.83it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1390038.74it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1315512.42it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1342472.92it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1242250.79it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1194568.54it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1254366.95it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1241731.86it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1280165.24it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1101609.12it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1303079.96it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1270985.94it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1254235.81it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1292517.81it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1279369.85it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1301024.79it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1221431.57it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1270321.22it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1249146.32it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1292674.81it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1257151.35it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1282392.01it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1251710.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52min 40s, sys: 21min 27s, total: 1h 14min 7s\n",
      "Wall time: 46min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_bg_dmm.train(utils.shuffle([x for x in tqdm(all_x_w2v_bg)]), total_examples=len(all_x_w2v_bg), epochs=1)\n",
    "    model_bg_dmm.alpha -= 0.002\n",
    "    model_bg_dmm.min_alpha = model_bg_dms.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_vecs_dmm_bg = get_vectors(model_bg_dmm, x_train, 100)\n",
    "validation_vecs_dmm_bg = get_vectors(model_bg_dmm, x_validation, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.3 s, sys: 8.41 s, total: 32.7 s\n",
      "Wall time: 36.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dmm_bg, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72863408521303263"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(validation_vecs_dmm_bg, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_bg_dmm.save('d2v_model_bg_dmm.doc2vec')\n",
    "model_bg_dmm = Doc2Vec.load('d2v_model_bg_dmm.doc2vec')\n",
    "model_bg_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_vecs_dbow_dmc_bg = get_concat_vectors(model_bg_dbow,model_bg_dmc, x_train, 200)\n",
    "validation_vecs_dbow_dmc_bg = get_concat_vectors(model_bg_dbow,model_bg_dmc, x_validation, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 30s, sys: 5min 26s, total: 6min 56s\n",
      "Wall time: 12min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dbow_dmc_bg, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74517543859649127"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(validation_vecs_dbow_dmc_bg, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_vecs_dbow_dmm_bg = get_concat_vectors(model_bg_dbow,model_bg_dmm, x_train, 200)\n",
    "validation_vecs_dbow_dmm_bg = get_concat_vectors(model_bg_dbow,model_bg_dmm, x_validation, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 57.3 s, sys: 1min 24s, total: 2min 21s\n",
      "Wall time: 3min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dbow_dmm_bg, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75369674185463664"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(validation_vecs_dbow_dmm_bg, y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we run the same phrase detection again on bigram detected corpus, now it will detect trigram phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 59s, sys: 14.8 s, total: 6min 14s\n",
      "Wall time: 6min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tg_phrases = Phrases(bigram[tokenized_train])\n",
    "trigram = Phraser(tg_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'last',\n",
       " u'time',\n",
       " u'with',\n",
       " u'nutella',\n",
       " u'and',\n",
       " u'vanilla_ice_cream',\n",
       " u'sadface']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram[bigram[x_train[627092].split()]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you remember that we saw the bigram model detected \"vanilla_ice\" with the above data entry? Now the trigram phrase modelling has detected \"vanilla_ice_cream\" as one word!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below Doc2Vec implementation is again same as unigram or bigram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def labelize_tweets_tg(tweets,label):\n",
    "    result = []\n",
    "    prefix = label\n",
    "    for i, t in zip(tweets.index, tweets):\n",
    "        result.append(LabeledSentence(trigram[bigram[t.split()]], [prefix + '_%s' % i]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_x = pd.concat([x_train,x_validation,x_test])\n",
    "all_x_w2v_tg = labelize_tweets_tg(all_x, 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## DBOW Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1596019/1596019 [00:01<00:00, 964801.43it/s]\n"
     ]
    }
   ],
   "source": [
    "model_tg_dbow = Doc2Vec(dm=0, size=100, negative=5, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model_tg_dbow.build_vocab([x for x in tqdm(all_x_w2v_tg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1140064.75it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1411727.82it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1400758.88it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1366421.73it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1370121.48it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1339086.85it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1122056.65it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1352223.71it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1356885.71it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1359952.95it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1394955.73it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1285871.94it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1269809.89it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1382792.32it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1291266.98it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1264850.59it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1320237.22it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1509095.09it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1502549.90it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1553715.73it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1533899.13it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1567101.09it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1559859.95it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1501313.19it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1570588.10it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1539189.09it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1494075.51it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1558860.31it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1516803.75it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1552153.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39min 9s, sys: 17min 47s, total: 56min 56s\n",
      "Wall time: 35min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_tg_dbow.train(utils.shuffle([x for x in tqdm(all_x_w2v_tg)]), total_examples=len(all_x_w2v_tg), epochs=1)\n",
    "    model_tg_dbow.alpha -= 0.002\n",
    "    model_tg_dbow.min_alpha = model_tg_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_vecs_dbow_tg = get_vectors(model_tg_dbow, x_train, 100)\n",
    "validation_vecs_dbow_tg = get_vectors(model_tg_dbow, x_validation, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24 s, sys: 8.83 s, total: 32.8 s\n",
      "Wall time: 37.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dbow_tg, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73684210526315785"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(validation_vecs_dbow_tg, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_tg_dbow.save('d2v_model_tg_dbow.doc2vec')\n",
    "model_tg_dbow = Doc2Vec.load('d2v_model_tg_dbow.doc2vec')\n",
    "model_tg_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## DMC Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1078282.64it/s]\n"
     ]
    }
   ],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "model_tg_dmc = Doc2Vec(dm=1, dm_concat=1, size=100, window=2, negative=5, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model_tg_dmc.build_vocab([x for x in tqdm(all_x_w2v_tg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1145472.61it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1300630.20it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1256518.95it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1209643.28it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1193364.50it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1319026.26it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1281172.21it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1255106.84it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1317832.32it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1416197.57it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1511951.37it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1418413.92it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1469892.19it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1452020.76it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1497197.89it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1518422.86it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1445674.57it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1405275.55it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1406458.03it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1486190.93it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1463165.27it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1480888.77it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1426145.25it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1477954.04it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1402711.36it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1490138.71it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1482590.65it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1271192.54it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1245809.45it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1286913.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 50min 45s, sys: 16min 55s, total: 1h 7min 41s\n",
      "Wall time: 39min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_tg_dmc.train(utils.shuffle([x for x in tqdm(all_x_w2v_tg)]), total_examples=len(all_x_w2v_tg), epochs=1)\n",
    "    model_tg_dmc.alpha -= 0.002\n",
    "    model_tg_dmc.min_alpha = model_tg_dmc.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_vecs_dmc_tg = get_vectors(model_tg_dmc, x_train, 100)\n",
    "validation_vecs_dmc_tg = get_vectors(model_tg_dmc, x_validation, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.8 s, sys: 13 s, total: 29.7 s\n",
      "Wall time: 36.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dmc_tg, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65507518796992481"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(validation_vecs_dmc_tg, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_tg_dmc.save('d2v_model_tg_dmc.doc2vec')\n",
    "model_tg_dmc = Doc2Vec.load('d2v_model_tg_dmc.doc2vec')\n",
    "model_tg_dmc.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## DMM Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1596019/1596019 [00:01<00:00, 884827.48it/s]\n"
     ]
    }
   ],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "model_tg_dmm = Doc2Vec(dm=1, dm_mean=1, size=100, window=4, negative=5, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model_tg_dmm.build_vocab([x for x in tqdm(all_x_w2v_tg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1012449.86it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1502780.28it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1470238.58it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1473524.71it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1525361.41it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1527416.60it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1470776.42it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1486240.76it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1272797.17it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1473264.95it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1486890.10it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1486375.07it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1498297.03it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1459908.90it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1541045.44it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1510792.22it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1288920.71it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1513289.14it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1244664.93it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1374330.97it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1156596.76it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1245845.38it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1225883.18it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1091598.74it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1257069.20it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1254488.48it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1263941.41it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1277456.01it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1232019.02it/s]\n",
      "100%|██████████| 1596019/1596019 [00:01<00:00, 1501218.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52min 31s, sys: 20min 33s, total: 1h 13min 4s\n",
      "Wall time: 47min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_tg_dmm.train(utils.shuffle([x for x in tqdm(all_x_w2v_tg)]), total_examples=len(all_x_w2v_tg), epochs=1)\n",
    "    model_tg_dmm.alpha -= 0.002\n",
    "    model_tg_dmc.min_alpha = model_tg_dmc.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_vecs_dmm_tg = get_vectors(model_tg_dmm, x_train, 100)\n",
    "validation_vecs_dmm_tg = get_vectors(model_tg_dmm, x_validation, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.5 s, sys: 12.6 s, total: 37.2 s\n",
      "Wall time: 43.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dmm_tg, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73840852130325818"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(validation_vecs_dmm_tg, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_tg_dmm.save('d2v_model_tg_dmm.doc2vec')\n",
    "model_tg_dmm = Doc2Vec.load('d2v_model_tg_dmm.doc2vec')\n",
    "model_tg_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_vecs_dbow_dmc_tg = get_concat_vectors(model_tg_dbow,model_tg_dmc, x_train, 200)\n",
    "validation_vecs_dbow_dmc_tg = get_concat_vectors(model_tg_dbow,model_tg_dmc, x_validation, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 53.2 s, sys: 1min 3s, total: 1min 56s\n",
      "Wall time: 3min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dbow_dmc_tg, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7461152882205514"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(validation_vecs_dbow_dmc_tg, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_vecs_dbow_dmm_tg = get_concat_vectors(model_tg_dbow,model_tg_dmm, x_train, 200)\n",
    "validation_vecs_dbow_dmm_tg = get_concat_vectors(model_tg_dbow,model_tg_dmm, x_validation, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 58s, sys: 15min 27s, total: 18min 26s\n",
      "Wall time: 39min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dbow_dmm_tg, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75657894736842102"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(validation_vecs_dbow_dmm_tg, y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we now have all the result from unigram to trigram and different Doc2Vec models, we can take a look at these results in a table format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validation set accuracy comparison of different Doc2Vec modeling:**\n",
    "*(classifier used for validation: logistic regression with default setting)*\n",
    "\n",
    "|            | unigram | bigram | trigram | best result      |\n",
    "|------------|---------|--------|---------|------------------|\n",
    "| DBOW       |  73.89% | 73.79% |  73.68% | 73.89% (unigram) |\n",
    "|  DMC       |  66.47% | 64.97% |  65.50% | 66.47% (unigram) |\n",
    "|  DMM       |  72.56% | 72.86% |  73.84% | 73.84% (trigram) |\n",
    "| dbow + dmc |  74.58% | 74.52% |  74.61% | 74.61% (trigram) |\n",
    "| dbow + dmm |  75.51% | 75.37% |  75.65% | 75.65% (trigram) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best validation accuracy I can get was from dbow+dmm model.\n",
    "\n",
    "DMM model tends to perform better with increased n-gram, while pure DBOW model tends to perform worse with increased n-gram. In terms of a joint model, two models performance got lower with bigram and got higher with trigram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before I move on to next step, I would like to try one more thing, which is creating joint vectors across different n-grams. By looking at the above table, for DBOW model unigram performed the best, so I will use vectors from unigram DBOW model and join this together with trigram DMM vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_ug_dbow = Doc2Vec.load('d2v_model_ug_dbow.doc2vec')\n",
    "model_tg_dmm = Doc2Vec.load('d2v_model_tg_dmm.doc2vec')\n",
    "model_ug_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "model_tg_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_vecs_ugdbow_tgdmm = get_concat_vectors(model_ug_dbow,model_tg_dmm, x_train, 200)\n",
    "validation_vecs_ugdbow_tgdmm = get_concat_vectors(model_ug_dbow,model_tg_dmm, x_validation, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 10s, sys: 43.6 s, total: 1min 54s\n",
      "Wall time: 2min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_ugdbow_tgdmm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75764411027568923"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(validation_vecs_ugdbow_tgdmm, y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is,\n",
    "\n",
    "unigram DBOW + trigram DMM: 75.76%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mmscaler = MinMaxScaler()\n",
    "d2v_ugdbow_tgdmm_mm = mmscaler.fit_transform(train_vecs_ugdbow_tgdmm)\n",
    "d2v_ugdbow_tgdmm_mm_val = mmscaler.fit_transform(validation_vecs_ugdbow_tgdmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names1 = [\"Logistic Regression\", \"Multinomial NB\", \n",
    "         \"Bernoulli NB\", \"Ridge Classifier\", \"Perceptron\",\"Passive-Aggresive\", \"Nearest Centroid\"]\n",
    "classifiers1 = [\n",
    "    LogisticRegression(),\n",
    "    MultinomialNB(),\n",
    "    BernoulliNB(),\n",
    "    RidgeClassifier(),\n",
    "    Perceptron(),\n",
    "    PassiveAggressiveClassifier(),\n",
    "    NearestCentroid()\n",
    "    ]\n",
    "zipped_clf1 = zip(names1,classifiers1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classifier_comparator_d2v(train_vectors,validation_vectors, classifier=zipped_clf1):\n",
    "    result = []\n",
    "    for n,c in classifier:\n",
    "        checker_pipeline = Pipeline([\n",
    "            ('classifier', c)\n",
    "        ])\n",
    "        print \"Validation result for {}\".format(n)\n",
    "        print c\n",
    "        clf_accuracy,tt_time = accuracy_summary(checker_pipeline, train_vectors, y_train, validation_vectors, y_validation)\n",
    "        result.append((n,clf_accuracy,tt_time))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation result for Logistic Regression\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "null accuracy: 50.40%\n",
      "accuracy score: 75.68%\n",
      "model is 25.28% more accurate than null accuracy\n",
      "train and test time: 154.56s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for Multinomial NB\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "null accuracy: 50.40%\n",
      "accuracy score: 73.04%\n",
      "model is 22.64% more accurate than null accuracy\n",
      "train and test time: 8.22s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for Bernoulli NB\n",
      "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "null accuracy: 50.40%\n",
      "accuracy score: 50.41%\n",
      "model is 0.01% more accurate than null accuracy\n",
      "train and test time: 16.53s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for Ridge Classifier\n",
      "RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
      "        max_iter=None, normalize=False, random_state=None, solver='auto',\n",
      "        tol=0.001)\n",
      "null accuracy: 50.40%\n",
      "accuracy score: 75.56%\n",
      "model is 25.16% more accurate than null accuracy\n",
      "train and test time: 20.16s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for Perceptron\n",
      "Perceptron(alpha=0.0001, class_weight=None, eta0=1.0, fit_intercept=True,\n",
      "      max_iter=5, n_iter=None, n_jobs=1, penalty=None, random_state=0,\n",
      "      shuffle=True, tol=None, verbose=0, warm_start=False)\n",
      "null accuracy: 50.40%\n",
      "accuracy score: 67.92%\n",
      "model is 17.52% more accurate than null accuracy\n",
      "train and test time: 7.32s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for Passive-Aggresive\n",
      "PassiveAggressiveClassifier(C=1.0, average=False, class_weight=None,\n",
      "              fit_intercept=True, loss='hinge', max_iter=5, n_iter=None,\n",
      "              n_jobs=1, random_state=None, shuffle=True, tol=None,\n",
      "              verbose=0, warm_start=False)\n",
      "null accuracy: 50.40%\n",
      "accuracy score: 62.16%\n",
      "model is 11.76% more accurate than null accuracy\n",
      "train and test time: 7.89s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for Nearest Centroid\n",
      "NearestCentroid(metric='euclidean', shrink_threshold=None)\n",
      "null accuracy: 50.40%\n",
      "accuracy score: 72.85%\n",
      "model is 22.45% more accurate than null accuracy\n",
      "train and test time: 1.84s\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Logistic Regression', 0.75676691729323309, 154.56316304206848),\n",
       " ('Multinomial NB', 0.73038847117794481, 8.222593069076538),\n",
       " ('Bernoulli NB', 0.50407268170426067, 16.526820182800293),\n",
       " ('Ridge Classifier', 0.7555764411027569, 20.15756106376648),\n",
       " ('Perceptron', 0.67919799498746869, 7.3240861892700195),\n",
       " ('Passive-Aggresive', 0.62161654135338351, 7.885602951049805),\n",
       " ('Nearest Centroid', 0.72850877192982455, 1.8390729427337646)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_comparator_d2v(d2v_ugdbow_tgdmm_mm,d2v_ugdbow_tgdmm_mm_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
